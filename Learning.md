### 1. [x] nanachiさん推奨の２位解答のwriteupsの概要を理解する
    - [writeups](./doc/writeups.mdx)

### 2. [x] 2位解答のtrainを動かす
    - [train](./train.py)
全データ, 4epoch
epoch,train_loss,valid_loss,f_th,time
0,nan,nan,0.00823403149843216,1:34:08
1,nan,nan,0.00823403149843216,1:33:14
2,nan,nan,0.00823403149843216,1:33:18
3,nan,nan,0.00823403149843216,1:33:15
size=10, 1epoch
epoch,train_loss,valid_loss,f_th,time
0,0.6441450119018555,0.6406621932983398,0.02800138108432293,01:05

### 3. [x] 単独モデルとして最高精度だったCoatについて、生成AIに練習問題を作ってもらって解く
    - [練習問題](./doc/understanding_challenges.md)

### 4. [x] 既存のCoaT_ULSTM.pyを使ったtrain.pyを写経する
    - [baseline](./base_train.py)
    - 判定：train.pyとの出力一致（augumentationにランダム要素あり）
size=10, 1epoch
epoch,train_loss,valid_loss,f_th,time
0,0.723307192325592,0.719579815864563,0.011882757768034935,01:05

### 5. [x] 既存のCoaT_ULSTM.pyを写経する
    - [baseline](./BaseCoatULSTM.py)
    - 判定：train.pyとの出力一致（augumentationにランダム要素あり）
size=10, 1epoch
epoch,train_loss,valid_loss,f_th,time
0,0.7154055237770081,0.7195807695388794,0.011882776394486427,01:04
全データ, 1epoch
epoch,train_loss,valid_loss,f_th,time
0,nan,0.04889664798974991,0.0,1:36:40

### 6. [x] 学習データを増やした時にtrain_lossが0になる原因を調査
train.py

size=100
0,0,0.558861,1.4e-05
0,10,0.569510,0.00018199999999999998
0,20,0.564075,0.00035
0,30,0.556591,0.00034734138261276724
0,40,0.548088,0.0003394463126188266
1,0,0.537310,0.0003265546787524436
1,10,0.520740,0.0003090581758511814
1,20,0.494278,0.0002874884512384853
1,30,0.468992,0.00026250087027019676
1,40,0.445649,0.00023485467312531644
2,0,0.424125,0.00020538985927895617
2,10,0.408211,0.0001750017172309627
2,20,0.393424,0.00014461360844984179
2,30,0.377378,0.00011514885766750733
2,40,0.363519,8.750264391921319e-05
3,0,0.350164,6.251504876151465e-05
3,10,0.341853,4.094530835116971e-05
3,20,0.334092,2.344880895916861e-05
3,30,0.326971,1.0557195786928784e-05
3,40,0.321159,2.662119165446888e-06

size=1000
Epoch 0, Batch 10: Loss=0.519364, LR=1.6068358533956484e-05------------------------------------------------------| 2.20% [11/500 01:04<47:42 0.5182]]
Epoch 0, Batch 20: Loss=0.520337, LR=2.222250429029033e-05-------------------------------------------------------| 4.20% [21/500 01:09<26:32 0.5196]
Epoch 0, Batch 30: Loss=0.522124, LR=3.2310901794069094e-05------------------------------------------------------| 6.20% [31/500 01:15<18:58 0.5220]
Epoch 0, Batch 40: Loss=0.520493, LR=4.608514124682983e-05-------------------------------------------------------| 8.20% [41/500 01:20<15:04 0.5207]
Epoch 0, Batch 50: Loss=0.521227, LR=6.320606076066002e-05-------------------------------------------------------| 10.20% [51/500 01:26<12:39 0.5217]
Epoch 0, Batch 60: Loss=0.513840, LR=8.325206997970443e-05-------------------------------------------------------| 12.20% [61/500 01:31<10:59 0.5152]
Epoch 0, Batch 70: Loss=0.499262, LR=0.0001057295932407752-------------------------------------------------------| 14.20% [71/500 01:37<09:46 0.5006]
Epoch 0, Batch 80: Loss=0.480296, LR=0.00013008513297745001------------------------------------------------------| 16.20% [81/500 01:42<08:50 0.4824]
Epoch 0, Batch 90: Loss=0.460888, LR=0.00015571901919462613------------------------------------------------------| 18.20% [91/500 01:48<08:05 0.4630]
Epoch 0, Batch 100: Loss=0.444492, LR=0.00018199999999999998-----------------------------------------------------| 20.20% [101/500 01:53<07:28 0.4455]
Epoch 0, Batch 110: Loss=0.428395, LR=0.0002082809963410153------------------------------------------------------| 22.20% [111/500 01:58<06:56 0.4303]
Epoch 0, Batch 120: Loss=0.408366, LR=0.00023391483710365242-----------------------------------------------------| 24.20% [121/500 02:04<06:29 0.4106]
Epoch 0, Batch 130: Loss=0.387406, LR=0.00025827039274431993-----------------------------------------------------| 26.20% [131/500 02:09<06:05 0.3896]
Epoch 0, Batch 140: Loss=0.366127, LR=0.0002807479172950286------------------------------------------------------| 28.20% [141/500 02:15<05:44 0.3684]
Epoch 0, Batch 150: Loss=0.350668, LR=0.00030079393923934--------------------------------------------------------| 30.20% [151/500 02:20<05:25 0.3521]
Epoch 0, Batch 160: Loss=0.334981, LR=0.00031791484026227326-----------------------------------------------------| 32.20% [161/500 02:26<05:08 0.3366]
Epoch 0, Batch 170: Loss=0.317035, LR=0.00033168910177640585-----------------------------------------------------| 34.20% [171/500 02:31<04:51 0.3190]
Epoch 0, Batch 180: Loss=0.301276, LR=0.0003417775005703282------------------------------------------------------| 36.20% [181/500 02:37<04:37 0.3031]
Epoch 0, Batch 190: Loss=nan, LR=0.0003479316402357415-----------------------------------------------------------| 38.20% [191/500 02:42<04:23 nan]18]
Epoch 0, Batch 200: Loss=nan, LR=0.00035█████████████------------------------------------------------------------| 40.20% [201/500 02:48<04:10 nan]
Epoch 0, Batch 210: Loss=nan, LR=0.0003499733469667587█----------------------------------------------------------| 42.20% [211/500 02:53<03:57 nan]
Epoch 0, Batch 220: Loss=nan, LR=0.0003498933958249705███--------------------------------------------------------| 44.20% [221/500 02:58<03:45 nan]
Epoch 0, Batch 230: Loss=nan, LR=0.0003497601709553542█████------------------------------------------------------| 46.20% [231/500 03:04<03:34 nan]

エポック１では発生しない場合もある

原因候補
1. OneCycle学習率と混合精度の相互作用
該当バッチでは学習率がピークに達しており、GradientAccumulation(16)で実効バッチが大きくなった状態でFP16演算を行う。自動LossScalingがあっても、瞬間的な出力ロジットの爆発でF.binary_cross_entropy_with_logits内にinftyが入り、smooth_lossがNaN化する典型パターンである。
対策: LR_MAXを半減して試行、to_fp16()を一度外してFP32で再現性を確認、またはGradientClip閾値を2.0以下に引き下げる。

-> LRのピーク付近でnanが発生しているので、一番ありそう
（pct_start=10%なのに、40%でピークになるのは謎）
対策1. LR_MAXを下げる
対策2. GradientClipを下げる
対策3. accumulationを減らす
筆者の環境との差分はaccumalationなので、対策3を試す

2. Lovaszヒンジ項のゼロ除算リスク
実装ではlovasz_gradの分母に微小項が入っておらず、予測・教師双方が空マスクのタイルで0/0が発生しNaNに遷移する。
対策: lovasz_gradの分母へ+1e-6を導入するか、空マスク時に0.を返すガードを追加する。

-> ありそう

3. 入力拡張と前処理の極端値
RandomGamma(50-150)とbicubic拡大により、ほぼ0/1に張り付いたテンソルが生成され、畳み込み直後の勾配が急峻化する。計算自体にNaNは含まれないが、急激な勾配と高LRが重なると前記の数値爆発を引き起こす。
対策: ガンマ補正の上限を下げる、学習初期だけ軽いオーグメンテーションに限定する。

-> augumentationにランダム要素はありそうだが、極端な値になるのか？

4. Over9000 (Ralamb + Lookahead) の信頼比率分岐
radam_normが極小化したステップではtrust_ratioが1にクリップされるためLossは守られるが、連続ステップで正規化回復が間に合わないと急激な重み更新が発生する。
対策: 一時的にWrapperAdamWへ切替え、挙動を比較する。

-> 正直理解できないが、ランダム要素はあるのか？

### 7. [x] CoaT_ULSTMの入出力、結果の可視化

- predictionがほぼ0、閾値を調整してもgroud truthと合わない

### 8. [x] ベースモデルで学習できるか確認

- [ResNet18_SimplerFCN](./src_inference1/ResNet18_SimplerFCN.py)
- 学習できていそう -> Coat_ULSTMの問題

### 9. [x] late submit

- 閾値探索を追加

#### 10. [ ] vast.ai用の学習スクリプト作成

- 学習実行手順
  - kaggle apiキーとwandb apiキーをscpで転送
  - 学習スクリプトを実行。以下が処理概要
    1. 公開コード、公開データセット、事前学習済みモデルをダウンロード
    2. データセットを展開
    3. 学習スクリプトを実行
    4. コードとモデルをkaggleにアップロード
- kaggle kernelでの動作確認とsubmitは手動